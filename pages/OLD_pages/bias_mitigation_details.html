<!DOCTYPE HTML>
<html lang="e">
<head>
	<title>Michael Baer: Projects</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="../../../../assets/css/main.css" />
	<noscript><link rel="stylesheet" href="../../../../assets/css/noscript.css" /></noscript>

	<style>
		* {
			box-sizing: border-box;
		}
		img {
			max-width: 100%;
			height: auto;
			margin-left: auto;
			margin-right: auto;
		}

		/* Create two equal columns that floats next to each other */
		.column {
			float: left;
		}
		.left {
			width: 50%;
		}

		.right {
			width: 50%;
		}

		/* Clear floats after the columns */
		.row:after {
			content: "";
			display: table;
			clear: both;
		}

		/* Responsive layout - makes the two columns stack on top of each other instead of next to each other */


		@media screen and (max-width: 736px) {
			.column {
				width: 100%;
			}
			img {
				max-height: 242px;
				margin-left: auto;
				margin-right: auto;
			}
		}
	</style>
</head>

<body class="is-preload">
<!-- Page Wrapper -->
<div id="page-wrapper">
	<!-- Header -->
	<header id="header">
		<h1><a href="../../../../index.html">Home</a></h1>
		<nav>
			<a href="#menu">Menu</a>
		</nav>
	</header>

	<!-- Menu -->
	<nav id="menu">
		<div class="inner">
			<h2>Menu</h2>
			<ul class="links">
				<li><a href="../../../../index.html">Home</a></li>
				<li><a href="../../../resume/resume.html">Resume</a></li>
				<li><a href="../../view_all.html">Projects</a></li>
			</ul>
			<a href="#" class="close">Close</a>
		</div>
	</nav>

	<!-- Wrapper -->
	<section id="wrapper">
		<header>
			<div class="inner">
				<h2>Loan Approval Dataset: Bias Mitigation for AI/ML Applications</h2>
				<p>This project examined the impact of using biased data that could be used to train algorithms
					associated with learning from credit-based data sets.
					Industry-defined fairness metrics were used to assess whether bias was present in the data.
					A bias-mitigating technique was then applied to produce a dataset without bias.</p>

			</div>
		</header>

		<!-- Content -->
		<div class="wrapper">
			<div class="inner">
				<h2>Background</h2>
				<p>Banks earn major revenue from lending loans, but this activity also brings risk as borrowers might default on their loan. To mitigate this, banks are leveraging Artificial Intelligence (AI) and Machine Learning (ML) to help predict how likely a borrower is to repay a loan. While this approach can lead to higher returns, banks must be careful when utilizing AI & ML for credit approval decisions due to the presence of bias in training data.</p>
				<p>For this project, I looked at the impact of using biased data that could be used to train
					algorithms associated with learning from credit-based data sets.

				<p>I worked on a Loan Approval dataset concerning credit card application decisions, which is interesting for AI & ML applications
				because there's a good mix of attributes -- continuous, nominal with a narrow range of values,
				and nominal with a wide range of values -- and it is subject to strong multi-collinearity & empty values.</p>

				<p>Of particular interest to me was the fact that this dataset also contained several protected class variables.
				I examined whether bias was present when looking at outcomes for 'rate_of_interest'
				and 'status' associated with different values for 'age' and 'gender' protected clase variables.
				I then applied bias-mitigating techniques to produce a dataset without bias.</p>
				<p>Identifying and mitigating bias within training data is necessary for AI applications for several reasons:</p>
				<ul>
					<li>Fairness: Today, AI applications are increasingly being used to make decisions that have a significant impact on people's lives, such as hiring, loan approvals, and parole decisions. If these applications are biased, they can unfairly discriminate against certain groups of people, leading to unjust outcomes.</li>
					<li>Accuracy: Inaccuracies in the predictions or recommendations generated by algorithm is another potential consequence of bias in AI applications. For example, if an AI algorithm is trained on biased data, it may make inaccurate predictions for individuals who don't fit the majority pattern in the data.</li>
					<li>Ethics: Discrimination and marginalization of underrepresented groups can perpetuate if AI applications contain bias. Mitigating bias is therefore an ethical imperative, as it helps ensure that AI is used in ways that are consistent with social and moral values.</li>
					<li>Trust: Bias can erode trust in AI applications, as users may be wary of using an algorithm that produces biased outcomes. By mitigating bias, AI developers can build trust in their applications and encourage greater adoption and usage.</li>
				</ul>
				<p>Overall, taking steps to mitigate bias is necessary to ensure that AI applications are fair, accurate, ethical, and trustworthy. This is essential for building AI systems that benefit society as a whole, rather than perpetuating existing inequalities and injustices.</p>
				<h2>Implementation</h2>
				<p>This "Fair Lending" Tool is a sophisticated Python-based software system that, given a dataset, can identify and mitigate biases related to protected attributes. It utilizes established fairness metrics and bias mitigation algorithms from the AI Fairness 360 (AIF360) toolkit to ensure a comprehensive and fair analysis.</p>

				<h4>Role</h4>
				<ul>
					<li><b>Data Scientist:</b> Responsible for designing the data processing pipeline, implementing bias detection and mitigation techniques, conducting evaluations, and ensuring the tool's requirements align with fairness principles.</li>
				</ul>

				<h4>Key Features</h4>
				<ul>
					<li><b>Jupyter Notebook Integration:</b> The entire project was developed in a Jupyter Notebook environment, a popular tool among data scientists for interactive data science and scientific computing across multiple programming languages.</li>
					<li><b>Data Preprocessing:</b> The initial step involved cleaning and formatting the dataset. This included handling missing values, converting data types, and transforming categorical variables into a suitable format for further analysis.</li>
					<li><b>Bias Detector:</b> Implemented a bias detection module using pandas and numpy to analyze the dataset and calculate disparity metrics.</li>
					<li><b>Bias Mitigator:</b> Developed a bias mitigator that applies fairness algorithms from the AIF360 toolkit to the data to reduce disparities.</li>
					<li><b>Visual Analysis:</b> The tool provides visual representation of disparity metrics before and after bias mitigation using matplotlib, aiding in understanding the effectiveness of the mitigation process.</li>
					<li><b>Statistical Literacy:</b> The tool integrates established fairness metrics and explanations to promote statistical literacy and empower users to make informed decisions.</li>
				</ul>
				<h4>Bias Mitigation Demonstration</h4>

				<div class="row">
					<div class="column left">
						<p><b>Preprocessing the Data:</b> The project kicked off with a thorough preprocessing phase where the raw dataset was transformed
							to be suitable for further analysis. This crucial step comprised the management of missing data and grouping of protected
							class variables into distinct subcategories. This procedure was essential to highlight potential
							biases in the data. All these preprocessing tasks were accomplished using powerful
							data manipulation libraries in Python, namely pandas and numpy. Their robust and efficient
							capabilities were instrumental in shaping the data into a format ready for insightful analysis.</p>
					</div>
					<div class="column right">
						<p><img src="../../../../assets/images/bias_mitigation/preprocessing1.PNG" alt="Dataset Preprocessing" title="Dataset Preprocessing"></p>
						<p><img src="../../../../assets/images/bias_mitigation/preprocessing2.PNG" alt="Dataset Preprocessing" title="Dataset Preprocessing"></p>

					</div>

				</div>


				<div class="row">
					<div class="column left">
						<p><b>Identifying the Bias:</b> After preprocessing, the project shifted to a focus on bias detection. Using fairness metrics such as Statistical Parity Difference (SPD) and Disparate Impact (DI), the dataset was scrutinized for any potential biases in relation to protected attributes like gender and age. These metrics were calculated using the AI Fairness 360 (AIF360) toolkit, a comprehensive library designed to help users examine, report, and mitigate discrimination and bias in machine learning models.</p>
					</div>


					<div class="column right">
						<p><img src="../../../../assets/images/bias_mitigation/bias_identification2.png" alt="Identifying Bias" title="Identifying Bias"></p>
					</div>
				</div>


				<div class="row">
					<div class="column left">
						<p><b>Mitigating the Bias:</b> The final stage of the project centered on bias mitigation. The Disparate Impact Remover (DIR) algorithm from the AIF360 toolkit was applied to the dataset, which works by adjusting the distributions of privileged and unprivileged groups for each feature to be more similar. The effectiveness of the bias mitigation process was then evaluated by calculating and comparing the fairness metrics before and after the mitigation. This comparison provided a clear visual representation of the reduction in bias, demonstrating the impact of the mitigation process.</p>
						<p>The transformed dataset mitigated bias for the 'derived_interest' column concerning both 'derived_age' and 'gender'. The dataset can now be used for more responsible AI training.</p>

					</div>


					<div class="column right">
						<p><img src="../../../../assets/images/bias_mitigation/bias_mitigation.PNG" alt="Mitigating Bias" title="Mitigating Bias"></p>
					</div>
				</div>

				<p>The program code can be found on my <a href="https://github.com/mbbaer/bias_mitigation" target="_blank">GitHub page</a>.</p>


			</div>
		</div>

	</section>



	<!-- Footer -->
	<section id="footer">
		<div class="inner">
			<ul class="copyright">
				<li>&copy; Michael Baer. All rights reserved.</li>
			</ul>
		</div>
	</section>
</div>

<!-- Scripts -->
<script src="../../../../assets/js/jquery.min.js"></script>
<script src="../../../../assets/js/jquery.scrollex.min.js"></script>
<script src="../../../../assets/js/browser.min.js"></script>
<script src="../../../../assets/js/breakpoints.min.js"></script>
<script src="../../../../assets/js/util.js"></script>
<script src="../../../../assets/js/main.js"></script>
</body>
</html>